# ğŸ¡ Zillow House Price Prediction  
Predicting Single-Family Home Sale Prices Using Zillowâ€™s Public Zestimate Dataset

This project builds an end-to-end machine learning workflow to predict house prices by minimizing **logerror**, the difference between Zillowâ€™s Zestimate and actual sale prices. The pipeline includes **data cleaning, feature engineering, model training, tuning, and evaluation**, with ensemble models demonstrating the strongest performance.

---

## ğŸ¯ Objective  
Accurately estimate home values and reduce logerror by leveraging:

- Property attributes (beds, baths, area, tax value, quality)
- Geolocation features (latitude, longitude, region)
- Engineered numerical and categorical variables
- Cross-validated model comparison with detailed error analysis

---

## ğŸ§  Key Results (Impact)

- **Ensemble models (Gradient Boosting / XGBoost) outperformed linear models**
- Achieved **~15% reduction in logerror** compared to baseline
- **~12% lower prediction variance** vs. linear models  
- Example performance:  
  - **Linear Regression RMSE â‰ˆ 0.085**  
  - **GBM/XGBoost RMSE â‰ˆ 0.0847**  
- Geospatial features significantly improved accuracy in high-data regions (e.g., **Los Angeles County â€” FIPS 6037**)

---

## ğŸ“¦ Data Sources

### **train_2016.csv**
- `parcelid`
- `transactiondate`
- `logerror` (target)

### **properties_2016.csv**
Contains ~60 features, including:
- Bedrooms, bathrooms
- Square footage (living area, lot size)
- Construction/quality metrics
- Land use codes
- Latitude & longitude
- Region, county, ZIP codes
- Tax amount & tax value

---

## ğŸ› ï¸ Tech Stack  
- Python  
- NumPy, Pandas  
- Scikit-learn  
- XGBoost, Gradient Boosting  
- Matplotlib, Seaborn  
- SciPy  
- Jupyter Notebook  

---

## ğŸ” Pipeline

### **1ï¸âƒ£ Exploratory Data Analysis (EDA)**
- Distribution inspection  
- Correlation heatmaps  
- Scatter plots against logerror  
- Categorical distributions (city, land use, quality)  
- Identified skewness, nonlinearity, and multicollinearity

---

### **2ï¸âƒ£ Feature Engineering**
- Duplicate removal  
- Missing value imputation:  
  - Mean/median/mode  
  - KNN Imputer  
  - MICE (Iterative)  
- Standardization and scaling  
- Categorical encoding:  
  - One-hot  
  - Label encoding  
  - Target encoding for high-cardinality  
- Interaction & time-based features  
- Outlier detection:  
  - IQR  
  - Z-score  
  - Isolation Forest  
- VIF checks for multicollinearity  
- Dropped low-variance, redundant fields

---

### **3ï¸âƒ£ Modeling**
Trained and compared:

- Linear Regression  
- Ridge / Lasso / ElasticNet  
- Decision Tree  
- Random Forest  
- AdaBoost  
- Gradient Boosting  
- **XGBoost (best result)**

**Metrics:**  
- RMSE  
- RÂ²  
- Cross-validation  
- Hyperparameter tuning  
  - e.g., `RandomForest(n_estimators=500, max_depth=6)`

---

### **4ï¸âƒ£ Evaluation & Interpretation**
- Residual plots  
- Error distribution analysis  
- Variance comparison  
- Feature importance ranking  
- Geospatial signal analysis  

---

## ğŸ” Top Predictors (Random Forest Importance)
Most influential features included:

- `taxamount`  
- `finishedsquarefeet12`  
- Latitude  
- Longitude  
- `lotsizesquarefeet`  
- Room counts  
- Zoning & land use variables  

These insights validated engineering choices and helped remove low-signal features.

---

## ğŸ“ˆ Cumulative Feature Importance
- **~80% predictive power** captured by top **7 features**  
- **~90%** captured by top **10** features  
- Diminishing returns after ~15  
- Trained optimized model using top-K features for stability + reduced runtime

---

## ğŸ“Š Representative Findings
- Ensemble models consistently achieved **lowest RMSE**  
- High-density counties (e.g., LA) provide more reliable predictions  
- Geospatial attributes played a critical role  
- Linear models struggled with nonlinear interactions and produced higher variance  

